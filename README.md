# JAX PRNG Reproducibility Playbook

A comprehensive **reproducibility framework** for JAX demonstrating industry best practices:

ğŸ”‘ **Core Features**:
- `jax.random.key(seed)` for explicit PRNG management (2025-style API)
- Process index folding for **multi-host safety** in distributed training
- Checkpoint/resume with **exact reproducibility** (save RNG state + model state)
- **Cryptographic hashing** of model parameters for verification
- Deterministic synthetic datasets with no external dependencies
- Complete tutorial with examples from beginner to expert level

ğŸ¯ **Perfect for**: Learning JAX reproducibility, debugging training runs, research reproducibility, production ML systems

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/mohsin-jax-labs/jax-prng-repro.git
cd jax-prng-repro

# Setup virtual environment  
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"

# Verify setup works
python verify_setup.py
```

### Basic Usage

#### 1) Test Reproducibility - Same seed = Same results
```bash
python -m jpr.run_repro --steps 50 --batch-size 64 --seed 0 --out runs/test1.json
python -m jpr.run_repro --steps 50 --batch-size 64 --seed 0 --out runs/test2.json
python -m jpr.compare runs/test1.json runs/test2.json
# Output: âœ… MATCH: hashes (and metrics if requested) are equal.
```

#### 2) Test Checkpoint/Resume - Interrupted = Uninterrupted  
```bash
# Full uninterrupted run
python -m jpr.run_repro --steps 60 --batch-size 64 --seed 123 --out runs/full.json

# Two-phase run with checkpoint  
python -m jpr.run_repro --steps 40 --batch-size 64 --seed 123 \
  --checkpoint-dir checkpoints/demo --save-at 40 --out runs/phase1.json

python -m jpr.run_repro --steps 60 --batch-size 64 --seed 123 \
  --checkpoint-dir checkpoints/demo --restore-from 40 --out runs/resumed.json

# Should be identical
python -m jpr.compare runs/full.json runs/resumed.json
# Output: âœ… MATCH: hashes (and metrics if requested) are equal.
```

## Learning Path

ğŸ“š **New to JAX reproducibility?** Follow our comprehensive tutorial:

1. **Start here**: [`TUTORIAL.md`](TUTORIAL.md) - Complete learning path
2. **Hands-on examples**: [`examples/`](examples/) directory  
3. **Usage guide**: [`examples/using_the_project.md`](examples/using_the_project.md)

## Architecture

This project demonstrates the **gold standard** for ML reproducibility:

```
ğŸ—ï¸ Core Framework:
â”œâ”€â”€ ğŸ² PRNG Management (rng.py) - Explicit key handling, multi-host safety
â”œâ”€â”€ ğŸ  Model (model.py) - Simple MLP with Flax + Optax  
â”œâ”€â”€ ğŸ“Š Data (data.py) - Deterministic synthetic dataset
â”œâ”€â”€ ğŸ’¾ Checkpointing (checkpoint.py) - Save model + RNG state
â”œâ”€â”€ ğŸ” Verification (hashing.py) - Parameter fingerprinting
â””â”€â”€ ğŸ”§ CLI Tools (run_repro.py, compare.py) - Training & comparison

ğŸ“š Complete Tutorial:
â”œâ”€â”€ examples/reproducibility_basics.py - Why reproducibility matters
â”œâ”€â”€ examples/jax_prng_tutorial.py - JAX vs NumPy randomness  
â”œâ”€â”€ examples/complete_flow.py - End-to-end demonstration
â”œâ”€â”€ examples/pitfalls_and_best_practices.py - Common mistakes
â””â”€â”€ examples/using_the_project.md - Usage guide
```

## Key Benefits

- âœ… **Perfect Reproducibility**: Same config â†’ Identical results  
- âœ… **Checkpoint Safety**: Resume = Continue uninterrupted
- âœ… **Multi-host Ready**: Distributed training support
- âœ… **Verification Built-in**: Cryptographic proof of equivalence
- âœ… **Production Ready**: Patterns used in real ML systems
- âœ… **Educational**: Learn from basics to expert level

## Technical Notes

- **Model**: Tiny Flax MLP (64â†’32â†’10) with Optax Adam optimizer
- **Data**: Synthetic classification dataset (deterministic, seed-controlled)  
- **Backends**: Tested on CPU/GPU (some numerical differences possible on edge cases)
- **Multi-host**: Uses `jax.process_index()` folding for distributed safety
- **Dependencies**: JAX â‰¥0.4.26, Flax â‰¥0.7, Optax â‰¥0.2.2, Orbax â‰¥0.6.1
